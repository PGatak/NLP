{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Na potrzeby tej lekcji utorzymy 2 foldery\n",
    "import os\n",
    "os.makedirs(r'nlp_task/source', exist_ok=True)\n",
    "os.makedirs(r'nlp_task/predictions', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-01-03 10:01:28--  http://mozart.ipipan.waw.pl/~alina/zasoby/data/COMBO-0.0.1.tar.gz\n",
      "Resolving mozart.ipipan.waw.pl (mozart.ipipan.waw.pl)... 213.135.36.148\n",
      "Connecting to mozart.ipipan.waw.pl (mozart.ipipan.waw.pl)|213.135.36.148|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 33675 (33K) [application/x-gzip]\n",
      "Saving to: ‘nlp_task/source/COMBO-0.0.1.tar.gz’\n",
      "\n",
      "COMBO-0.0.1.tar.gz  100%[===================>]  32,89K  --.-KB/s    in 0,05s   \n",
      "\n",
      "2021-01-03 10:01:28 (705 KB/s) - ‘nlp_task/source/COMBO-0.0.1.tar.gz’ saved [33675/33675]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -P nlp_task/source http://mozart.ipipan.waw.pl/~alina/zasoby/data/COMBO-0.0.1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: urllib3==1.25.11 in /home/adsum/.local/lib/python3.6/site-packages (1.25.11)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install urllib3==1.25.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Processing ./nlp_task/source/COMBO-0.0.1.tar.gz\n",
      "Requirement already satisfied: urllib3>=1.25.11 in /home/adsum/.local/lib/python3.6/site-packages (from COMBO==0.0.1) (1.25.11)\n",
      "Collecting absl-py==0.9.0\n",
      "  Downloading absl-py-0.9.0.tar.gz (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /home/adsum/.local/lib/python3.6/site-packages (from absl-py==0.9.0->COMBO==0.0.1) (1.15.0)\n",
      "Collecting allennlp==1.2.0\n",
      "  Downloading allennlp-1.2.0-py3-none-any.whl (498 kB)\n",
      "\u001b[K     |████████████████████████████████| 498 kB 7.4 MB/s eta 0:00:01     |██████████████████████▍         | 348 kB 7.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py in /home/adsum/.local/lib/python3.6/site-packages (from allennlp==1.2.0->COMBO==0.0.1) (2.10.0)\n",
      "Requirement already satisfied: spacy<2.4,>=2.1.0 in /home/adsum/.local/lib/python3.6/site-packages (from allennlp==1.2.0->COMBO==0.0.1) (2.3.2)\n",
      "Requirement already satisfied: pytest in /home/adsum/.local/lib/python3.6/site-packages (from allennlp==1.2.0->COMBO==0.0.1) (6.1.1)\n",
      "Requirement already satisfied: nltk in /home/adsum/.local/lib/python3.6/site-packages (from allennlp==1.2.0->COMBO==0.0.1) (3.5)\n",
      "Requirement already satisfied: numpy in /home/adsum/.local/lib/python3.6/site-packages (from allennlp==1.2.0->COMBO==0.0.1) (1.19.2)\n",
      "Requirement already satisfied: scipy in /home/adsum/.local/lib/python3.6/site-packages (from allennlp==1.2.0->COMBO==0.0.1) (1.5.4)\n",
      "Requirement already satisfied: scikit-learn in /home/adsum/.local/lib/python3.6/site-packages (from allennlp==1.2.0->COMBO==0.0.1) (0.21.3)\n",
      "Collecting conllu==2.3.2\n",
      "  Downloading conllu-2.3.2-py2.py3-none-any.whl (13 kB)\n",
      "Collecting dataclasses-json==0.5.2\n",
      "  Downloading dataclasses_json-0.5.2-py3-none-any.whl (23 kB)\n",
      "Collecting joblib==0.14.1\n",
      "  Using cached joblib-0.14.1-py2.py3-none-any.whl (294 kB)\n",
      "Collecting jsonnet==0.15.0\n",
      "  Downloading jsonnet-0.15.0.tar.gz (255 kB)\n",
      "\u001b[K     |████████████████████████████████| 255 kB 7.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting overrides==3.1.0\n",
      "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
      "Collecting requests==2.23.0\n",
      "  Using cached requests-2.23.0-py2.py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/adsum/.local/lib/python3.6/site-packages (from requests==2.23.0->COMBO==0.0.1) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/adsum/.local/lib/python3.6/site-packages (from requests==2.23.0->COMBO==0.0.1) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/adsum/.local/lib/python3.6/site-packages (from requests==2.23.0->COMBO==0.0.1) (3.0.4)\n",
      "Collecting stringcase<2.0.0,==1.2.0\n",
      "  Downloading stringcase-1.2.0.tar.gz (3.0 kB)\n",
      "Collecting tensorboard==2.1.0\n",
      "  Downloading tensorboard-2.1.0-py3-none-any.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-auth<2,>=1.6.3 in /home/adsum/.local/lib/python3.6/site-packages (from tensorboard==2.1.0->COMBO==0.0.1) (1.24.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/adsum/.local/lib/python3.6/site-packages (from tensorboard==2.1.0->COMBO==0.0.1) (3.3.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/adsum/.local/lib/python3.6/site-packages (from tensorboard==2.1.0->COMBO==0.0.1) (51.1.0.post20201221)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard==2.1.0->COMBO==0.0.1) (0.30.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/adsum/.local/lib/python3.6/site-packages (from tensorboard==2.1.0->COMBO==0.0.1) (1.34.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/lib/python3/dist-packages (from tensorboard==2.1.0->COMBO==0.0.1) (0.14.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/adsum/.local/lib/python3.6/site-packages (from tensorboard==2.1.0->COMBO==0.0.1) (3.14.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/adsum/.local/lib/python3.6/site-packages (from tensorboard==2.1.0->COMBO==0.0.1) (0.4.2)\n",
      "Collecting torch==1.6.0\n",
      "  Downloading torch-1.6.0-cp36-cp36m-manylinux1_x86_64.whl (748.8 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 748.8 MB 7.7 kB/s eta 0:00:018  |                                | 337 kB 3.2 MB/s eta 0:03:54     |█▏                              | 26.3 MB 987 kB/s eta 0:12:12     |█▏                              | 27.3 MB 987 kB/s eta 0:12:11     |█▍                              | 31.4 MB 1.6 MB/s eta 0:07:21     |█▍                              | 32.8 MB 1.6 MB/s eta 0:07:29     |██▋                             | 62.1 MB 2.2 MB/s eta 0:05:07     |██▊                             | 64.6 MB 2.7 MB/s eta 0:04:13     |████                            | 92.3 MB 2.4 MB/s eta 0:04:31     |████▎                           | 100.4 MB 5.6 MB/s eta 0:01:55     |████▊                           | 110.8 MB 1.6 MB/s eta 0:06:48     |█████▏                          | 119.8 MB 6.2 MB/s eta 0:01:42     |██████▍                         | 149.3 MB 4.5 MB/s eta 0:02:15     |██████▌                         | 152.7 MB 2.9 MB/s eta 0:03:25     |███████                         | 164.4 MB 5.2 MB/s eta 0:01:53     |███████▍                        | 172.6 MB 1.4 MB/s eta 0:06:45     |████████                        | 187.6 MB 4.5 MB/s eta 0:02:04     |████████▏                       | 191.1 MB 4.6 MB/s eta 0:02:01     |████████▍                       | 195.7 MB 1.3 MB/s eta 0:06:54     |████████▌                       | 198.9 MB 3.4 MB/s eta 0:02:42     |█████████                       | 213.1 MB 5.9 MB/s eta 0:01:31     |█████████▎                      | 216.3 MB 5.7 MB/s eta 0:01:35     |█████████▍                      | 220.3 MB 5.7 MB/s eta 0:01:34     |█████████▋                      | 226.0 MB 4.9 MB/s eta 0:01:46     |██████████                      | 234.9 MB 6.7 MB/s eta 0:01:17     |██████████▋                     | 247.6 MB 3.3 MB/s eta 0:02:34     |███████████▏                    | 261.6 MB 4.7 MB/s eta 0:01:43     |███████████▎                    | 265.0 MB 2.3 MB/s eta 0:03:33     |███████████▉                    | 277.6 MB 2.8 MB/s eta 0:02:50     |███████████▉                    | 277.8 MB 2.8 MB/s eta 0:02:50     |████████████▉                   | 301.1 MB 4.1 MB/s eta 0:01:50     |█████████████▉                  | 322.9 MB 4.0 MB/s eta 0:01:46     |██████████████████▊             | 438.3 MB 1.6 MB/s eta 0:03:10     |██████████████████▉             | 441.3 MB 1.6 MB/s eta 0:03:08     |███████████████████▍            | 452.8 MB 6.2 MB/s eta 0:00:49     |███████████████████▊            | 462.7 MB 5.7 MB/s eta 0:00:51     |███████████████████▉            | 464.7 MB 5.7 MB/s eta 0:00:51     |████████████████████            | 465.9 MB 4.4 MB/s eta 0:01:04     |████████████████████            | 469.6 MB 5.9 MB/s eta 0:00:48     |████████████████████▏           | 471.0 MB 5.9 MB/s eta 0:00:48     |████████████████████▌           | 478.8 MB 3.8 MB/s eta 0:01:12     |█████████████████████▌          | 502.4 MB 6.7 MB/s eta 0:00:37     |█████████████████████▌          | 503.9 MB 1.9 MB/s eta 0:02:08     |██████████████████████          | 514.6 MB 6.2 MB/s eta 0:00:39     |███████████████████████▋        | 553.7 MB 1.7 MB/s eta 0:01:58     |███████████████████████▊        | 556.1 MB 1.7 MB/s eta 0:01:57     |███████████████████████▉        | 558.0 MB 1.7 MB/s eta 0:01:56     |████████████████████████        | 562.4 MB 2.7 MB/s eta 0:01:10     |████████████████████████        | 563.5 MB 4.6 MB/s eta 0:00:41     |████████████████████████▋       | 575.5 MB 4.3 MB/s eta 0:00:41     |█████████████████████████▋      | 599.6 MB 4.8 MB/s eta 0:00:31     |██████████████████████████▋     | 623.4 MB 1.3 MB/s eta 0:01:35     |██████████████████████████▉     | 627.3 MB 4.5 MB/s eta 0:00:27     |███████████████████████████▎    | 639.3 MB 3.4 MB/s eta 0:00:33     |████████████████████████████    | 653.3 MB 6.5 MB/s eta 0:00:15     |████████████████████████████▍   | 664.2 MB 11.3 MB/s eta 0:00:08     |████████████████████████████▍   | 664.6 MB 6.9 MB/s eta 0:00:13     |████████████████████████████▌   | 666.7 MB 6.9 MB/s eta 0:00:12     |████████████████████████████▌   | 667.9 MB 6.9 MB/s eta 0:00:12     |████████████████████████████▊   | 672.1 MB 3.8 MB/s eta 0:00:20     |████████████████████████████▉   | 673.8 MB 3.8 MB/s eta 0:00:20     |█████████████████████████████▌  | 690.3 MB 6.1 MB/s eta 0:00:10     |██████████████████████████████▋ | 716.6 MB 7.5 MB/s eta 0:00:05     |███████████████████████████████▍| 734.1 MB 6.0 MB/s eta 0:00:03     |███████████████████████████████▊| 742.6 MB 3.8 MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: future in /home/adsum/.local/lib/python3.6/site-packages (from torch==1.6.0->COMBO==0.0.1) (0.18.2)\n",
      "Collecting tqdm==4.43.0\n",
      "  Downloading tqdm-4.43.0-py2.py3-none-any.whl (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 5.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting boto3<2.0,>=1.14\n",
      "  Downloading boto3-1.16.47-py2.py3-none-any.whl (130 kB)\n",
      "\u001b[K     |████████████████████████████████| 130 kB 5.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting botocore<1.20.0,>=1.19.47\n",
      "  Downloading botocore-1.19.47-py2.py3-none-any.whl (7.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.2 MB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/adsum/.local/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.47->boto3<2.0,>=1.14->allennlp==1.2.0->COMBO==0.0.1) (2.8.1)\n",
      "Collecting filelock<3.1,>=3.0\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.1.0->COMBO==0.0.1) (0.2.6)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/adsum/.local/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard==2.1.0->COMBO==0.0.1) (4.6)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/adsum/.local/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard==2.1.0->COMBO==0.0.1) (4.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/adsum/.local/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.1.0->COMBO==0.0.1) (1.3.0)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: importlib-metadata in /home/adsum/.local/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard==2.1.0->COMBO==0.0.1) (2.0.0)\n",
      "Collecting marshmallow<4.0.0,>=3.3.0\n",
      "  Downloading marshmallow-3.10.0-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 3.7 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1\n",
      "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard==2.1.0->COMBO==0.0.1) (0.4.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/adsum/.local/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.1.0->COMBO==0.0.1) (3.1.0)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/adsum/.local/lib/python3.6/site-packages (from spacy<2.4,>=2.1.0->allennlp==1.2.0->COMBO==0.0.1) (3.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/adsum/.local/lib/python3.6/site-packages (from spacy<2.4,>=2.1.0->allennlp==1.2.0->COMBO==0.0.1) (2.0.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/adsum/.local/lib/python3.6/site-packages (from spacy<2.4,>=2.1.0->allennlp==1.2.0->COMBO==0.0.1) (0.4.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/adsum/.local/lib/python3.6/site-packages (from spacy<2.4,>=2.1.0->allennlp==1.2.0->COMBO==0.0.1) (1.0.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/adsum/.local/lib/python3.6/site-packages (from spacy<2.4,>=2.1.0->allennlp==1.2.0->COMBO==0.0.1) (1.1.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/adsum/.local/lib/python3.6/site-packages (from spacy<2.4,>=2.1.0->allennlp==1.2.0->COMBO==0.0.1) (0.8.0)\n",
      "Requirement already satisfied: thinc==7.4.1 in /home/adsum/.local/lib/python3.6/site-packages (from spacy<2.4,>=2.1.0->allennlp==1.2.0->COMBO==0.0.1) (7.4.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/adsum/.local/lib/python3.6/site-packages (from spacy<2.4,>=2.1.0->allennlp==1.2.0->COMBO==0.0.1) (1.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/adsum/.local/lib/python3.6/site-packages (from spacy<2.4,>=2.1.0->allennlp==1.2.0->COMBO==0.0.1) (1.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/adsum/.local/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard==2.1.0->COMBO==0.0.1) (3.3.2)\n",
      "Collecting tensorboardX>=1.2\n",
      "  Downloading tensorboardX-2.1-py2.py3-none-any.whl (308 kB)\n",
      "\u001b[K     |████████████████████████████████| 308 kB 6.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting transformers<3.5,>=3.4.0\n",
      "  Downloading transformers-3.4.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 7.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/adsum/.local/lib/python3.6/site-packages (from transformers<3.5,>=3.4.0->COMBO==0.0.1) (20.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/adsum/.local/lib/python3.6/site-packages (from transformers<3.5,>=3.4.0->COMBO==0.0.1) (2020.11.13)\n",
      "Collecting tokenizers==0.9.2\n",
      "  Downloading tokenizers-0.9.2-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 4.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
      "  Downloading sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 5.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typing-inspect>=0.4.0\n",
      "  Downloading typing_inspect-0.6.0-py3-none-any.whl (8.1 kB)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Using cached mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
      "Collecting typing-extensions>=3.7.4\n",
      "  Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting dataclasses\n",
      "  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Collecting jsonpickle\n",
      "  Downloading jsonpickle-1.4.2-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: click in /home/adsum/.local/lib/python3.6/site-packages (from nltk->allennlp==1.2.0->COMBO==0.0.1) (7.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/adsum/.local/lib/python3.6/site-packages (from packaging->transformers<3.5,>=3.4.0->COMBO==0.0.1) (2.4.7)\n",
      "Requirement already satisfied: iniconfig in /home/adsum/.local/lib/python3.6/site-packages (from pytest->allennlp==1.2.0->COMBO==0.0.1) (1.0.1)\n",
      "Requirement already satisfied: py>=1.8.2 in /home/adsum/.local/lib/python3.6/site-packages (from pytest->allennlp==1.2.0->COMBO==0.0.1) (1.9.0)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in /home/adsum/.local/lib/python3.6/site-packages (from pytest->allennlp==1.2.0->COMBO==0.0.1) (0.13.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/adsum/.local/lib/python3.6/site-packages (from pytest->allennlp==1.2.0->COMBO==0.0.1) (20.2.0)\n",
      "Requirement already satisfied: toml in /home/adsum/.local/lib/python3.6/site-packages (from pytest->allennlp==1.2.0->COMBO==0.0.1) (0.10.1)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "\u001b[K     |████████████████████████████████| 883 kB 8.8 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: COMBO, absl-py, jsonnet, overrides, stringcase, sacremoses\n",
      "  Building wheel for COMBO (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for COMBO: filename=COMBO-0.0.1-py3-none-any.whl size=45890 sha256=02efd130862ea832e8f858f7cf4f455547f787c3d65097699cf979d6e9d42e5b\n",
      "  Stored in directory: /home/adsum/.cache/pip/wheels/54/da/51/1b81d56d9b1a6e70df88175b94a863fd2984ae4437862a11a1\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=119396 sha256=cee84e0e60135adade8ff29ddb65c1acef6c5034eb0a9316782db3414a61a150\n",
      "  Stored in directory: /home/adsum/.cache/pip/wheels/c3/af/84/3962a6af7b4ab336e951b7877dcfb758cf94548bb1771e0679\n",
      "  Building wheel for jsonnet (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jsonnet: filename=jsonnet-0.15.0-cp36-cp36m-linux_x86_64.whl size=3316396 sha256=b8c015c421ad43abfbf809e8636fa7c1d7ebcd3d4bac0afeaac0cc568ea2e4c5\n",
      "  Stored in directory: /home/adsum/.cache/pip/wheels/74/70/67/749ed86d16e27859685124771aeba5eda85035c2017a20fdf9\n",
      "  Building wheel for overrides (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=7981 sha256=176a3c6420ad1186a5e9cbb0bbb7232dcf5d1b9c936f43c1ad18e99332269964\n",
      "  Stored in directory: /home/adsum/.cache/pip/wheels/e6/3b/34/ae59fc8d35c37f01099425ab73599e45e9b9b599a7ccc2c45f\n",
      "  Building wheel for stringcase (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for stringcase: filename=stringcase-1.2.0-py3-none-any.whl size=4111 sha256=d6e8e2aae9a6f9ec609502beda7bd63aaad994e265bd2c5d1bde00a9dc313f3c\n",
      "  Stored in directory: /home/adsum/.cache/pip/wheels/39/ed/1a/d1fbf258dd3b80cf68bdcc602c60f46039c4aacd9ae672b827\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=894090 sha256=d2bc613715c1a4bb2b377731de2bebddb43178b0b8b2d11189dda04a58855f34\n",
      "  Stored in directory: /home/adsum/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\n",
      "Successfully built COMBO absl-py jsonnet overrides stringcase sacremoses\n",
      "Installing collected packages: jmespath, tqdm, requests, joblib, botocore, typing-extensions, tokenizers, sentencepiece, sacremoses, s3transfer, mypy-extensions, marshmallow, filelock, dataclasses, typing-inspect, transformers, torch, tensorboardX, stringcase, overrides, marshmallow-enum, jsonpickle, jsonnet, boto3, absl-py, tensorboard, dataclasses-json, conllu, allennlp, COMBO\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.54.1\n",
      "    Uninstalling tqdm-4.54.1:\n",
      "      Successfully uninstalled tqdm-4.54.1\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.24.0\n",
      "    Uninstalling requests-2.24.0:\n",
      "      Successfully uninstalled requests-2.24.0\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.0.0\n",
      "    Uninstalling joblib-1.0.0:\n",
      "      Successfully uninstalled joblib-1.0.0\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 0.11.0\n",
      "    Uninstalling absl-py-0.11.0:\n",
      "      Successfully uninstalled absl-py-0.11.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.4.0\n",
      "    Uninstalling tensorboard-2.4.0:\n",
      "      Successfully uninstalled tensorboard-2.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.3.1 requires numpy<1.19.0,>=1.16.0, but you have numpy 1.19.2 which is incompatible.\n",
      "tensorflow 2.3.1 requires tensorboard<3,>=2.3.0, but you have tensorboard 2.1.0 which is incompatible.\u001b[0m\n",
      "Successfully installed COMBO-0.0.1 absl-py-0.9.0 allennlp-1.2.0 boto3-1.16.47 botocore-1.19.47 conllu-2.3.2 dataclasses-0.8 dataclasses-json-0.5.2 filelock-3.0.12 jmespath-0.10.0 joblib-0.17.0 jsonnet-0.15.0 jsonpickle-1.4.2 marshmallow-3.10.0 marshmallow-enum-1.5.1 mypy-extensions-0.4.3 overrides-3.1.0 requests-2.23.0 s3transfer-0.3.3 sacremoses-0.0.43 sentencepiece-0.1.94 stringcase-1.2.0 tensorboard-2.1.0 tensorboardX-2.1 tokenizers-0.9.2 torch-1.6.0 tqdm-4.50.2 transformers-3.4.0 typing-extensions-3.7.4.3 typing-inspect-0.6.0\n"
     ]
    }
   ],
   "source": [
    "! python3 -m pip install nlp_task/source/COMBO-0.0.1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import combo.predict as predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32708/32708 [02:18<00:00, 236.93it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9fc92c4fd1f42709a004664e8de0247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=464.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e747cdca856b46da92e5ebe1d5d6dfe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=906984.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d9112c4be564f658e4ec6ee295f2795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=555571.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7306ab1ae3da4d2a9d0b39a142140be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=129.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4db15ed1584634b4bd5370151da9e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=229.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e77992c59bd4ae982285b4fb6d98dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=654201076.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = predict.SemanticMultitaskPredictor.from_pretrained(\"polish-herbert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Kupiliśmy nowy samochód.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEGMENT              LEMAT                UPOS            XPOS                 FEATS          \n",
      "Kupiliśmy            kupilić              VERB            fin:pl:pri:perf      Aspect=Perf|Mood=Ind|Number=Plur|Person=1|Tense=Fut|VerbForm=Fin|Voice=Act\n",
      "nowy                 nowy                 ADJ             adj:sg:acc:m3:pos    Animacy=Inan|Case=Acc|Degree=Pos|Gender=Masc|Number=Sing\n",
      "samochód             samochód             NOUN            subst:sg:acc:m3      Animacy=Inan|Case=Acc|Gender=Masc|Number=Sing\n",
      ".                    .                    PUNCT           interp               PunctType=Peri \n"
     ]
    }
   ],
   "source": [
    "print(\"{:20} {:20} {:15} {:20} {:15}\".format('SEGMENT', 'LEMAT', 'UPOS', 'XPOS', 'FEATS'))\n",
    "for token in doc.tokens:\n",
    "    print(\"{:20} {:20} {:15} {:20} {:15}\".format(token.token, token.lemma, token.upostag, token.xpostag, token.feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-01-03 10:39:04--  http://mozart.ipipan.waw.pl/~mklimaszewski/models/polish-herbert-base.tar.gz\n",
      "Resolving mozart.ipipan.waw.pl (mozart.ipipan.waw.pl)... 213.135.36.148\n",
      "Connecting to mozart.ipipan.waw.pl (mozart.ipipan.waw.pl)|213.135.36.148|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 334925644 (319M) [application/x-gzip]\n",
      "Saving to: ‘nlp_task/source/polish-herbert-base.tar.gz’\n",
      "\n",
      "polish-herbert-base 100%[===================>] 319,41M  3,17MB/s    in 2m 3s   \n",
      "\n",
      "2021-01-03 10:41:07 (2,60 MB/s) - ‘nlp_task/source/polish-herbert-base.tar.gz’ saved [334925644/334925644]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget -P nlp_task/source http://mozart.ipipan.waw.pl/~mklimaszewski/models/polish-herbert-base.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-01-03 10:41:07--  http://mozart.ipipan.waw.pl/~alina/zasoby/data/PDBUD_test.conllu\n",
      "Resolving mozart.ipipan.waw.pl (mozart.ipipan.waw.pl)... 213.135.36.148\n",
      "Connecting to mozart.ipipan.waw.pl (mozart.ipipan.waw.pl)|213.135.36.148|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3486186 (3,3M)\n",
      "Saving to: ‘nlp_task/source/PDBUD_test.conllu’\n",
      "\n",
      "PDBUD_test.conllu   100%[===================>]   3,32M  2,58MB/s    in 1,3s    \n",
      "\n",
      "2021-01-03 10:41:09 (2,58 MB/s) - ‘nlp_task/source/PDBUD_test.conllu’ saved [3486186/3486186]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget -P nlp_task/source http://mozart.ipipan.waw.pl/~alina/zasoby/data/PDBUD_test.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-01-03 10:41:09--  https://wolnelektury.pl/media/book/txt/korczak-krol-macius-na-wyspie-bezludnej.txt\n",
      "Resolving wolnelektury.pl (wolnelektury.pl)... 51.83.143.148, 2001:41d0:602:3294::\n",
      "Connecting to wolnelektury.pl (wolnelektury.pl)|51.83.143.148|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 326911 (319K) [text/plain]\n",
      "Saving to: ‘nlp_task/source/korczak-krol-macius-na-wyspie-bezludnej.txt.3’\n",
      "\n",
      "korczak-krol-macius 100%[===================>] 319,25K  --.-KB/s    in 0,09s   \n",
      "\n",
      "2021-01-03 10:41:10 (3,34 MB/s) - ‘nlp_task/source/korczak-krol-macius-na-wyspie-bezludnej.txt.3’ saved [326911/326911]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -P nlp_task/source https://wolnelektury.pl/media/book/txt/korczak-krol-macius-na-wyspie-bezludnej.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Janusz Korczak\r",
      "\r\n",
      "\r",
      "\r\n",
      "Król Maciuś na wyspie bezludnej\r",
      "\r\n",
      "\r",
      "\r\n",
      "ISBN 978-83-288-2376-1\r",
      "\r\n",
      "\r",
      "\r\n",
      "\r",
      "\r\n",
      "Ach, jak strasznie źle było Maciusiowi w więzieniu. Właściwie nie było mu źle — ale ciasno. Ciasno i nudno.\r",
      "\r\n",
      "\r",
      "\r\n",
      "Bo jeżeli ktoś siedzi w więzieniu — i mają rozstrzelać — wcale się nie nudzi. Ale Maciuś miał być wysłany na bezludną wyspę. Maciuś przegrał wojnę, był jeńcem królewskim i — zupełnie jak Napoleon będzie wysłany na wyspę.\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 nlp_task/source/korczak-krol-macius-na-wyspie-bezludnej.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-03 10:45:29.934643: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2021-01-03 10:45:29.934724: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "I0103 10:45:42.975383 139963894171456 archival.py:184] loading archive file nlp_task/source/polish-herbert-base.tar.gz\n",
      "I0103 10:45:42.976653 139963894171456 archival.py:263] extracting archive file nlp_task/source/polish-herbert-base.tar.gz to temp dir /tmp/tmpog9bag7p\n",
      "I0103 10:46:01.935956 139963894171456 params.py:248] dataset_reader.type = conllu\n",
      "I0103 10:46:01.937571 139963894171456 params.py:248] dataset_reader.lazy = False\n",
      "I0103 10:46:01.938010 139963894171456 params.py:248] dataset_reader.cache_directory = None\n",
      "I0103 10:46:01.938350 139963894171456 params.py:248] dataset_reader.max_instances = None\n",
      "I0103 10:46:01.938629 139963894171456 params.py:248] dataset_reader.manual_distributed_sharding = False\n",
      "I0103 10:46:01.938883 139963894171456 params.py:248] dataset_reader.manual_multi_process_sharding = False\n",
      "I0103 10:46:01.939700 139963894171456 params.py:248] dataset_reader.token_indexers.char.type = characters_const_padding\n",
      "I0103 10:46:01.940316 139963894171456 params.py:248] dataset_reader.token_indexers.char.namespace = token_characters\n",
      "I0103 10:46:01.941010 139963894171456 params.py:248] dataset_reader.token_indexers.char.character_tokenizer.byte_encoding = None\n",
      "I0103 10:46:01.941251 139963894171456 params.py:248] dataset_reader.token_indexers.char.character_tokenizer.lowercase_characters = False\n",
      "I0103 10:46:01.941591 139963894171456 params.py:248] dataset_reader.token_indexers.char.character_tokenizer.start_tokens = ['__START__']\n",
      "I0103 10:46:01.942279 139963894171456 params.py:248] dataset_reader.token_indexers.char.character_tokenizer.end_tokens = ['__END__']\n",
      "I0103 10:46:01.942856 139963894171456 params.py:248] dataset_reader.token_indexers.char.start_tokens = None\n",
      "I0103 10:46:01.943069 139963894171456 params.py:248] dataset_reader.token_indexers.char.end_tokens = None\n",
      "I0103 10:46:01.943349 139963894171456 params.py:248] dataset_reader.token_indexers.char.min_padding_length = 32\n",
      "I0103 10:46:01.943587 139963894171456 params.py:248] dataset_reader.token_indexers.char.token_min_padding_length = 0\n",
      "I0103 10:46:01.944091 139963894171456 params.py:248] dataset_reader.token_indexers.feats.type = feats_indexer\n",
      "I0103 10:46:01.944658 139963894171456 params.py:248] dataset_reader.token_indexers.feats.namespace = feats\n",
      "I0103 10:46:01.944870 139963894171456 params.py:248] dataset_reader.token_indexers.feats.feature_name = feats_\n",
      "I0103 10:46:01.945162 139963894171456 params.py:248] dataset_reader.token_indexers.feats.token_min_padding_length = 0\n",
      "I0103 10:46:01.946018 139963894171456 params.py:248] dataset_reader.token_indexers.lemma.type = characters_const_padding\n",
      "I0103 10:46:01.946631 139963894171456 params.py:248] dataset_reader.token_indexers.lemma.namespace = token_characters\n",
      "I0103 10:46:01.947218 139963894171456 params.py:248] dataset_reader.token_indexers.lemma.character_tokenizer.byte_encoding = None\n",
      "I0103 10:46:01.947418 139963894171456 params.py:248] dataset_reader.token_indexers.lemma.character_tokenizer.lowercase_characters = False\n",
      "I0103 10:46:01.947625 139963894171456 params.py:248] dataset_reader.token_indexers.lemma.character_tokenizer.start_tokens = ['__START__']\n",
      "I0103 10:46:01.948112 139963894171456 params.py:248] dataset_reader.token_indexers.lemma.character_tokenizer.end_tokens = ['__END__']\n",
      "I0103 10:46:01.948650 139963894171456 params.py:248] dataset_reader.token_indexers.lemma.start_tokens = None\n",
      "I0103 10:46:01.948854 139963894171456 params.py:248] dataset_reader.token_indexers.lemma.end_tokens = None\n",
      "I0103 10:46:01.949101 139963894171456 params.py:248] dataset_reader.token_indexers.lemma.min_padding_length = 32\n",
      "I0103 10:46:01.949683 139963894171456 params.py:248] dataset_reader.token_indexers.lemma.token_min_padding_length = 0\n",
      "I0103 10:46:01.951159 139963894171456 params.py:248] dataset_reader.token_indexers.token.type = pretrained_transformer_mismatched_fixed\n",
      "I0103 10:46:01.952190 139963894171456 params.py:248] dataset_reader.token_indexers.token.token_min_padding_length = 0\n",
      "I0103 10:46:01.952687 139963894171456 params.py:248] dataset_reader.token_indexers.token.model_name = allegro/herbert-base-cased\n",
      "I0103 10:46:01.953264 139963894171456 params.py:248] dataset_reader.token_indexers.token.namespace = tags\n",
      "I0103 10:46:01.953669 139963894171456 params.py:248] dataset_reader.token_indexers.token.max_length = None\n",
      "I0103 10:46:01.954372 139963894171456 params.py:384] dataset_reader.token_indexers.token.tokenizer_kwargs.use_fast = False\n",
      "I0103 10:46:12.000115 139963894171456 params.py:248] dataset_reader.token_indexers.upostag.type = single_id\n",
      "I0103 10:46:12.003122 139963894171456 params.py:248] dataset_reader.token_indexers.upostag.namespace = upostag\n",
      "I0103 10:46:12.003939 139963894171456 params.py:248] dataset_reader.token_indexers.upostag.lowercase_tokens = False\n",
      "I0103 10:46:12.004991 139963894171456 params.py:248] dataset_reader.token_indexers.upostag.start_tokens = None\n",
      "I0103 10:46:12.005774 139963894171456 params.py:248] dataset_reader.token_indexers.upostag.end_tokens = None\n",
      "I0103 10:46:12.006430 139963894171456 params.py:248] dataset_reader.token_indexers.upostag.feature_name = pos_\n",
      "I0103 10:46:12.006996 139963894171456 params.py:248] dataset_reader.token_indexers.upostag.default_value = THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING\n",
      "I0103 10:46:12.007410 139963894171456 params.py:248] dataset_reader.token_indexers.upostag.token_min_padding_length = 0\n",
      "I0103 10:46:12.008442 139963894171456 params.py:248] dataset_reader.token_indexers.xpostag.type = single_id\n",
      "I0103 10:46:12.009931 139963894171456 params.py:248] dataset_reader.token_indexers.xpostag.namespace = xpostag\n",
      "I0103 10:46:12.010463 139963894171456 params.py:248] dataset_reader.token_indexers.xpostag.lowercase_tokens = False\n",
      "I0103 10:46:12.010915 139963894171456 params.py:248] dataset_reader.token_indexers.xpostag.start_tokens = None\n",
      "I0103 10:46:12.011317 139963894171456 params.py:248] dataset_reader.token_indexers.xpostag.end_tokens = None\n",
      "I0103 10:46:12.011761 139963894171456 params.py:248] dataset_reader.token_indexers.xpostag.feature_name = tag_\n",
      "I0103 10:46:12.012352 139963894171456 params.py:248] dataset_reader.token_indexers.xpostag.default_value = THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING\n",
      "I0103 10:46:12.013055 139963894171456 params.py:248] dataset_reader.token_indexers.xpostag.token_min_padding_length = 0\n",
      "I0103 10:46:12.014916 139963894171456 params.py:248] dataset_reader.lemma_indexers.char.type = characters_const_padding\n",
      "I0103 10:46:12.015827 139963894171456 params.py:248] dataset_reader.lemma_indexers.char.namespace = lemma_characters\n",
      "I0103 10:46:12.016681 139963894171456 params.py:248] dataset_reader.lemma_indexers.char.character_tokenizer.byte_encoding = None\n",
      "I0103 10:46:12.016973 139963894171456 params.py:248] dataset_reader.lemma_indexers.char.character_tokenizer.lowercase_characters = False\n",
      "I0103 10:46:12.017301 139963894171456 params.py:248] dataset_reader.lemma_indexers.char.character_tokenizer.start_tokens = ['__START__']\n",
      "I0103 10:46:12.018039 139963894171456 params.py:248] dataset_reader.lemma_indexers.char.character_tokenizer.end_tokens = ['__END__']\n",
      "I0103 10:46:12.018568 139963894171456 params.py:248] dataset_reader.lemma_indexers.char.start_tokens = None\n",
      "I0103 10:46:12.018778 139963894171456 params.py:248] dataset_reader.lemma_indexers.char.end_tokens = None\n",
      "I0103 10:46:12.019019 139963894171456 params.py:248] dataset_reader.lemma_indexers.char.min_padding_length = 32\n",
      "I0103 10:46:12.019191 139963894171456 params.py:248] dataset_reader.lemma_indexers.char.token_min_padding_length = 0\n",
      "I0103 10:46:12.019467 139963894171456 params.py:248] dataset_reader.features = ['token', 'char']\n",
      "I0103 10:46:12.019756 139963894171456 params.py:248] dataset_reader.targets = ['deprel', 'feats', 'head', 'lemma', 'upostag', 'xpostag']\n",
      "I0103 10:46:12.020054 139963894171456 params.py:248] dataset_reader.use_sem = False\n",
      "I0103 10:46:12.020491 139963894171456 params.py:248] dataset_reader.type = conllu\n",
      "I0103 10:46:12.021269 139963894171456 params.py:248] dataset_reader.lazy = False\n",
      "I0103 10:46:12.021487 139963894171456 params.py:248] dataset_reader.cache_directory = None\n",
      "I0103 10:46:12.021683 139963894171456 params.py:248] dataset_reader.max_instances = None\n",
      "I0103 10:46:12.021847 139963894171456 params.py:248] dataset_reader.manual_distributed_sharding = False\n",
      "I0103 10:46:12.022011 139963894171456 params.py:248] dataset_reader.manual_multi_process_sharding = False\n",
      "I0103 10:46:12.022708 139963894171456 params.py:248] dataset_reader.token_indexers.char.type = characters_const_padding\n",
      "I0103 10:46:12.023244 139963894171456 params.py:248] dataset_reader.token_indexers.char.namespace = token_characters\n",
      "I0103 10:46:12.023822 139963894171456 params.py:248] dataset_reader.token_indexers.char.character_tokenizer.byte_encoding = None\n",
      "I0103 10:46:12.024028 139963894171456 params.py:248] dataset_reader.token_indexers.char.character_tokenizer.lowercase_characters = False\n",
      "I0103 10:46:12.024270 139963894171456 params.py:248] dataset_reader.token_indexers.char.character_tokenizer.start_tokens = ['__START__']\n",
      "I0103 10:46:12.024762 139963894171456 params.py:248] dataset_reader.token_indexers.char.character_tokenizer.end_tokens = ['__END__']\n",
      "I0103 10:46:12.025797 139963894171456 params.py:248] dataset_reader.token_indexers.char.start_tokens = None\n",
      "I0103 10:46:12.026072 139963894171456 params.py:248] dataset_reader.token_indexers.char.end_tokens = None\n",
      "I0103 10:46:12.026285 139963894171456 params.py:248] dataset_reader.token_indexers.char.min_padding_length = 32\n",
      "I0103 10:46:12.026530 139963894171456 params.py:248] dataset_reader.token_indexers.char.token_min_padding_length = 0\n",
      "I0103 10:46:12.027059 139963894171456 params.py:248] dataset_reader.token_indexers.feats.type = feats_indexer\n",
      "I0103 10:46:12.027726 139963894171456 params.py:248] dataset_reader.token_indexers.feats.namespace = feats\n",
      "I0103 10:46:12.028039 139963894171456 params.py:248] dataset_reader.token_indexers.feats.feature_name = feats_\n",
      "I0103 10:46:12.028252 139963894171456 params.py:248] dataset_reader.token_indexers.feats.token_min_padding_length = 0\n",
      "I0103 10:46:12.028786 139963894171456 params.py:248] dataset_reader.token_indexers.lemma.type = characters_const_padding\n",
      "I0103 10:46:12.029452 139963894171456 params.py:248] dataset_reader.token_indexers.lemma.namespace = token_characters\n",
      "I0103 10:46:12.030250 139963894171456 params.py:248] dataset_reader.token_indexers.lemma.character_tokenizer.byte_encoding = None\n",
      "I0103 10:46:12.030484 139963894171456 params.py:248] dataset_reader.token_indexers.lemma.character_tokenizer.lowercase_characters = False\n",
      "I0103 10:46:12.030702 139963894171456 params.py:248] dataset_reader.token_indexers.lemma.character_tokenizer.start_tokens = ['__START__']\n",
      "I0103 10:46:12.031275 139963894171456 params.py:248] dataset_reader.token_indexers.lemma.character_tokenizer.end_tokens = ['__END__']\n",
      "I0103 10:46:12.031762 139963894171456 params.py:248] dataset_reader.token_indexers.lemma.start_tokens = None\n",
      "I0103 10:46:12.031945 139963894171456 params.py:248] dataset_reader.token_indexers.lemma.end_tokens = None\n",
      "I0103 10:46:12.032139 139963894171456 params.py:248] dataset_reader.token_indexers.lemma.min_padding_length = 32\n",
      "I0103 10:46:12.032303 139963894171456 params.py:248] dataset_reader.token_indexers.lemma.token_min_padding_length = 0\n",
      "I0103 10:46:12.032750 139963894171456 params.py:248] dataset_reader.token_indexers.token.type = pretrained_transformer_mismatched_fixed\n",
      "I0103 10:46:12.033370 139963894171456 params.py:248] dataset_reader.token_indexers.token.token_min_padding_length = 0\n",
      "I0103 10:46:12.033578 139963894171456 params.py:248] dataset_reader.token_indexers.token.model_name = allegro/herbert-base-cased\n",
      "I0103 10:46:12.033743 139963894171456 params.py:248] dataset_reader.token_indexers.token.namespace = tags\n",
      "I0103 10:46:12.033904 139963894171456 params.py:248] dataset_reader.token_indexers.token.max_length = None\n",
      "I0103 10:46:12.034155 139963894171456 params.py:384] dataset_reader.token_indexers.token.tokenizer_kwargs.use_fast = False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0103 10:46:12.048636 139963894171456 params.py:248] dataset_reader.token_indexers.upostag.type = single_id\n",
      "I0103 10:46:12.049543 139963894171456 params.py:248] dataset_reader.token_indexers.upostag.namespace = upostag\n",
      "I0103 10:46:12.049958 139963894171456 params.py:248] dataset_reader.token_indexers.upostag.lowercase_tokens = False\n",
      "I0103 10:46:12.050368 139963894171456 params.py:248] dataset_reader.token_indexers.upostag.start_tokens = None\n",
      "I0103 10:46:12.050730 139963894171456 params.py:248] dataset_reader.token_indexers.upostag.end_tokens = None\n",
      "I0103 10:46:12.051017 139963894171456 params.py:248] dataset_reader.token_indexers.upostag.feature_name = pos_\n",
      "I0103 10:46:12.051218 139963894171456 params.py:248] dataset_reader.token_indexers.upostag.default_value = THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING\n",
      "I0103 10:46:12.051385 139963894171456 params.py:248] dataset_reader.token_indexers.upostag.token_min_padding_length = 0\n",
      "I0103 10:46:12.051874 139963894171456 params.py:248] dataset_reader.token_indexers.xpostag.type = single_id\n",
      "I0103 10:46:12.052411 139963894171456 params.py:248] dataset_reader.token_indexers.xpostag.namespace = xpostag\n",
      "I0103 10:46:12.052617 139963894171456 params.py:248] dataset_reader.token_indexers.xpostag.lowercase_tokens = False\n",
      "I0103 10:46:12.052787 139963894171456 params.py:248] dataset_reader.token_indexers.xpostag.start_tokens = None\n",
      "I0103 10:46:12.052945 139963894171456 params.py:248] dataset_reader.token_indexers.xpostag.end_tokens = None\n",
      "I0103 10:46:12.053108 139963894171456 params.py:248] dataset_reader.token_indexers.xpostag.feature_name = tag_\n",
      "I0103 10:46:12.053242 139963894171456 params.py:248] dataset_reader.token_indexers.xpostag.default_value = THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING\n",
      "I0103 10:46:12.053398 139963894171456 params.py:248] dataset_reader.token_indexers.xpostag.token_min_padding_length = 0\n",
      "I0103 10:46:12.053976 139963894171456 params.py:248] dataset_reader.lemma_indexers.char.type = characters_const_padding\n",
      "I0103 10:46:12.054503 139963894171456 params.py:248] dataset_reader.lemma_indexers.char.namespace = lemma_characters\n",
      "I0103 10:46:12.055241 139963894171456 params.py:248] dataset_reader.lemma_indexers.char.character_tokenizer.byte_encoding = None\n",
      "I0103 10:46:12.055530 139963894171456 params.py:248] dataset_reader.lemma_indexers.char.character_tokenizer.lowercase_characters = False\n",
      "I0103 10:46:12.055895 139963894171456 params.py:248] dataset_reader.lemma_indexers.char.character_tokenizer.start_tokens = ['__START__']\n",
      "I0103 10:46:12.056633 139963894171456 params.py:248] dataset_reader.lemma_indexers.char.character_tokenizer.end_tokens = ['__END__']\n",
      "I0103 10:46:12.057295 139963894171456 params.py:248] dataset_reader.lemma_indexers.char.start_tokens = None\n",
      "I0103 10:46:12.057581 139963894171456 params.py:248] dataset_reader.lemma_indexers.char.end_tokens = None\n",
      "I0103 10:46:12.057917 139963894171456 params.py:248] dataset_reader.lemma_indexers.char.min_padding_length = 32\n",
      "I0103 10:46:12.058234 139963894171456 params.py:248] dataset_reader.lemma_indexers.char.token_min_padding_length = 0\n",
      "I0103 10:46:12.058687 139963894171456 params.py:248] dataset_reader.features = ['token', 'char']\n",
      "I0103 10:46:12.059671 139963894171456 params.py:248] dataset_reader.targets = ['deprel', 'feats', 'head', 'lemma', 'upostag', 'xpostag']\n",
      "I0103 10:46:12.060247 139963894171456 params.py:248] dataset_reader.use_sem = False\n",
      "I0103 10:46:12.062836 139963894171456 params.py:248] vocabulary.type = from_instances_extended\n",
      "I0103 10:46:12.063326 139963894171456 vocabulary.py:323] Loading token dictionary from /tmp/tmpog9bag7p/vocabulary.\n",
      "I0103 10:46:12.067890 139963894171456 filelock.py:274] Lock 139960400428728 acquired on /tmp/tmpog9bag7p/vocabulary/.lock\n",
      "I0103 10:46:12.073383 139963894171456 filelock.py:318] Lock 139960400428728 released on /tmp/tmpog9bag7p/vocabulary/.lock\n",
      "I0103 10:46:12.076427 139963894171456 params.py:248] model.type = semantic_multitask\n",
      "I0103 10:46:12.078497 139963894171456 params.py:248] model.text_field_embedder.type = basic\n",
      "I0103 10:46:12.080497 139963894171456 params.py:248] model.text_field_embedder.token_embedders.char.type = char_embeddings_from_config\n",
      "I0103 10:46:12.081238 139963894171456 params.py:248] model.text_field_embedder.token_embedders.char.embedding_dim = 64\n",
      "I0103 10:46:12.082459 139963894171456 params.py:248] model.text_field_embedder.token_embedders.char.dilated_cnn_encoder.input_dim = 64\n",
      "I0103 10:46:12.082992 139963894171456 params.py:248] model.text_field_embedder.token_embedders.char.dilated_cnn_encoder.filters = [512, 256, 64]\n",
      "I0103 10:46:12.083406 139963894171456 params.py:248] model.text_field_embedder.token_embedders.char.dilated_cnn_encoder.kernel_size = [3, 3, 3]\n",
      "I0103 10:46:12.084175 139963894171456 params.py:248] model.text_field_embedder.token_embedders.char.dilated_cnn_encoder.stride = [1, 1, 1]\n",
      "I0103 10:46:12.084758 139963894171456 params.py:248] model.text_field_embedder.token_embedders.char.dilated_cnn_encoder.padding = [1, 2, 4]\n",
      "I0103 10:46:12.085158 139963894171456 params.py:248] model.text_field_embedder.token_embedders.char.dilated_cnn_encoder.dilation = [1, 2, 4]\n",
      "I0103 10:46:12.085477 139963894171456 params.py:248] model.text_field_embedder.token_embedders.char.dilated_cnn_encoder.activations = ['relu', 'relu', 'linear']\n",
      "I0103 10:46:12.086258 139963894171456 params.py:248] type = relu\n",
      "I0103 10:46:12.087035 139963894171456 params.py:248] type = relu\n",
      "I0103 10:46:12.087565 139963894171456 params.py:248] type = linear\n",
      "I0103 10:46:12.117660 139963894171456 params.py:248] model.text_field_embedder.token_embedders.char.vocab_namespace = token_characters\n",
      "I0103 10:46:12.119770 139963894171456 params.py:248] model.text_field_embedder.token_embedders.token.type = transformers_word_embeddings\n",
      "I0103 10:46:12.120636 139963894171456 params.py:248] model.text_field_embedder.token_embedders.token.model_name = allegro/herbert-base-cased\n",
      "I0103 10:46:12.121002 139963894171456 params.py:248] model.text_field_embedder.token_embedders.token.projection_dim = 100\n",
      "I0103 10:46:12.121375 139963894171456 params.py:248] model.text_field_embedder.token_embedders.token.projection_activation = <function TransformersWordEmbedder.<lambda> at 0x7f4b11f19ea0>\n",
      "I0103 10:46:12.121707 139963894171456 params.py:248] model.text_field_embedder.token_embedders.token.projection_dropout_rate = 0.0\n",
      "I0103 10:46:12.121983 139963894171456 params.py:248] model.text_field_embedder.token_embedders.token.freeze_transformer = True\n",
      "I0103 10:46:12.122319 139963894171456 params.py:384] model.text_field_embedder.token_embedders.token.tokenizer_kwargs.use_fast = False\n",
      "I0103 10:46:12.122529 139963894171456 params.py:248] model.text_field_embedder.token_embedders.token.transformer_kwargs = None\n",
      "I0103 10:46:24.383788 139963894171456 params.py:248] model.seq_encoder.type = combo_encoder\n",
      "I0103 10:46:24.385560 139963894171456 params.py:248] model.seq_encoder.stacked_bilstm.input_size = 164\n",
      "I0103 10:46:24.386095 139963894171456 params.py:248] model.seq_encoder.stacked_bilstm.hidden_size = 512\n",
      "I0103 10:46:24.386424 139963894171456 params.py:248] model.seq_encoder.stacked_bilstm.num_layers = 2\n",
      "I0103 10:46:24.386822 139963894171456 params.py:248] model.seq_encoder.stacked_bilstm.recurrent_dropout_probability = 0.33\n",
      "I0103 10:46:24.387152 139963894171456 params.py:248] model.seq_encoder.stacked_bilstm.layer_dropout_probability = 0.33\n",
      "I0103 10:46:24.387495 139963894171456 params.py:248] model.seq_encoder.stacked_bilstm.use_highway = False\n",
      "I0103 10:46:26.539967 139963894171456 params.py:248] model.seq_encoder.layer_dropout_probability = 0.33\n",
      "I0103 10:46:26.540984 139963894171456 params.py:248] model.use_sample_weight = True\n",
      "I0103 10:46:26.543242 139963894171456 params.py:248] model.lemmatizer.type = combo_lemma_predictor_from_vocab\n",
      "I0103 10:46:26.549224 139963894171456 params.py:248] model.lemmatizer.char_vocab_namespace = token_characters\n",
      "I0103 10:46:26.550208 139963894171456 params.py:248] model.lemmatizer.lemma_vocab_namespace = lemma_characters\n",
      "I0103 10:46:26.551723 139963894171456 params.py:248] model.lemmatizer.embedding_dim = 256\n",
      "I0103 10:46:26.553651 139963894171456 params.py:248] model.lemmatizer.input_projection_layer.in_features = 1024\n",
      "I0103 10:46:26.554697 139963894171456 params.py:248] model.lemmatizer.input_projection_layer.out_features = 32\n",
      "I0103 10:46:26.555150 139963894171456 params.py:248] model.lemmatizer.input_projection_layer.activation = tanh\n",
      "I0103 10:46:26.555978 139963894171456 params.py:248] type = tanh\n",
      "I0103 10:46:26.556665 139963894171456 params.py:248] model.lemmatizer.input_projection_layer.dropout_rate = 0.25\n",
      "I0103 10:46:26.559166 139963894171456 params.py:248] model.lemmatizer.filters = [256, 256, 256]\n",
      "I0103 10:46:26.560361 139963894171456 params.py:248] model.lemmatizer.kernel_size = [3, 3, 3, 1]\n",
      "I0103 10:46:26.561358 139963894171456 params.py:248] model.lemmatizer.stride = [1, 1, 1, 1]\n",
      "I0103 10:46:26.561933 139963894171456 params.py:248] model.lemmatizer.padding = [1, 2, 4, 0]\n",
      "I0103 10:46:26.562429 139963894171456 params.py:248] model.lemmatizer.dilation = [1, 2, 4, 1]\n",
      "I0103 10:46:26.562962 139963894171456 params.py:248] model.lemmatizer.activations = ['relu', 'relu', 'relu', 'linear']\n",
      "I0103 10:46:26.563818 139963894171456 params.py:248] type = relu\n",
      "I0103 10:46:26.564977 139963894171456 params.py:248] type = relu\n",
      "I0103 10:46:26.565924 139963894171456 params.py:248] type = relu\n",
      "I0103 10:46:26.566735 139963894171456 params.py:248] type = linear\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0103 10:46:26.598655 139963894171456 params.py:248] model.upos_tagger.type = feedforward_predictor_from_vocab\n",
      "I0103 10:46:26.600449 139963894171456 params.py:248] model.upos_tagger.vocab_namespace = upostag_labels\n",
      "I0103 10:46:26.601031 139963894171456 params.py:248] model.upos_tagger.input_dim = 1024\n",
      "I0103 10:46:26.601359 139963894171456 params.py:248] model.upos_tagger.num_layers = 2\n",
      "I0103 10:46:26.601721 139963894171456 params.py:248] model.upos_tagger.hidden_dims = [64]\n",
      "I0103 10:46:26.602181 139963894171456 params.py:248] model.upos_tagger.activations = ['tanh', 'linear']\n",
      "I0103 10:46:26.604095 139963894171456 params.py:248] type = tanh\n",
      "I0103 10:46:26.604970 139963894171456 params.py:248] type = linear\n",
      "I0103 10:46:26.605784 139963894171456 params.py:248] model.upos_tagger.dropout = [0.25, 0]\n",
      "I0103 10:46:26.612279 139963894171456 params.py:248] model.xpos_tagger.type = feedforward_predictor_from_vocab\n",
      "I0103 10:46:26.613400 139963894171456 params.py:248] model.xpos_tagger.vocab_namespace = xpostag_labels\n",
      "I0103 10:46:26.613824 139963894171456 params.py:248] model.xpos_tagger.input_dim = 1024\n",
      "I0103 10:46:26.614146 139963894171456 params.py:248] model.xpos_tagger.num_layers = 2\n",
      "I0103 10:46:26.614434 139963894171456 params.py:248] model.xpos_tagger.hidden_dims = [128]\n",
      "I0103 10:46:26.614719 139963894171456 params.py:248] model.xpos_tagger.activations = ['tanh', 'linear']\n",
      "I0103 10:46:26.615791 139963894171456 params.py:248] type = tanh\n",
      "I0103 10:46:26.616475 139963894171456 params.py:248] type = linear\n",
      "I0103 10:46:26.616902 139963894171456 params.py:248] model.xpos_tagger.dropout = [0.25, 0]\n",
      "I0103 10:46:26.627053 139963894171456 params.py:248] model.semantic_relation = None\n",
      "I0103 10:46:26.628706 139963894171456 params.py:248] model.morphological_feat.type = combo_morpho_from_vocab\n",
      "I0103 10:46:26.629558 139963894171456 params.py:248] model.morphological_feat.vocab_namespace = feats_labels\n",
      "I0103 10:46:26.629816 139963894171456 params.py:248] model.morphological_feat.input_dim = 1024\n",
      "I0103 10:46:26.630068 139963894171456 params.py:248] model.morphological_feat.num_layers = 2\n",
      "I0103 10:46:26.630346 139963894171456 params.py:248] model.morphological_feat.hidden_dims = [128]\n",
      "I0103 10:46:26.630672 139963894171456 params.py:248] model.morphological_feat.activations = ['tanh', 'linear']\n",
      "I0103 10:46:26.631911 139963894171456 params.py:248] type = tanh\n",
      "I0103 10:46:26.632488 139963894171456 params.py:248] type = linear\n",
      "I0103 10:46:26.632899 139963894171456 params.py:248] model.morphological_feat.dropout = [0.25, 0]\n",
      "I0103 10:46:26.639980 139963894171456 params.py:248] model.dependency_relation.type = combo_dependency_parsing_from_vocab\n",
      "I0103 10:46:26.640869 139963894171456 params.py:248] model.dependency_relation.vocab_namespace = deprel_labels\n",
      "I0103 10:46:26.642185 139963894171456 params.py:248] model.dependency_relation.head_predictor.head_projection_layer.in_features = 1024\n",
      "I0103 10:46:26.642893 139963894171456 params.py:248] model.dependency_relation.head_predictor.head_projection_layer.out_features = 512\n",
      "I0103 10:46:26.643276 139963894171456 params.py:248] model.dependency_relation.head_predictor.head_projection_layer.activation = tanh\n",
      "I0103 10:46:26.644057 139963894171456 params.py:248] type = tanh\n",
      "I0103 10:46:26.644461 139963894171456 params.py:248] model.dependency_relation.head_predictor.head_projection_layer.dropout_rate = 0.0\n",
      "I0103 10:46:26.657907 139963894171456 params.py:248] model.dependency_relation.head_predictor.dependency_projection_layer.in_features = 1024\n",
      "I0103 10:46:26.658603 139963894171456 params.py:248] model.dependency_relation.head_predictor.dependency_projection_layer.out_features = 512\n",
      "I0103 10:46:26.659130 139963894171456 params.py:248] model.dependency_relation.head_predictor.dependency_projection_layer.activation = tanh\n",
      "I0103 10:46:26.659656 139963894171456 params.py:248] type = tanh\n",
      "I0103 10:46:26.660104 139963894171456 params.py:248] model.dependency_relation.head_predictor.dependency_projection_layer.dropout_rate = 0.0\n",
      "I0103 10:46:26.678538 139963894171456 params.py:248] model.dependency_relation.head_predictor.cycle_loss_n = 0\n",
      "I0103 10:46:26.680420 139963894171456 params.py:248] model.dependency_relation.head_projection_layer.in_features = 1024\n",
      "I0103 10:46:26.680833 139963894171456 params.py:248] model.dependency_relation.head_projection_layer.out_features = 128\n",
      "I0103 10:46:26.681167 139963894171456 params.py:248] model.dependency_relation.head_projection_layer.activation = tanh\n",
      "I0103 10:46:26.681781 139963894171456 params.py:248] type = tanh\n",
      "I0103 10:46:26.682346 139963894171456 params.py:248] model.dependency_relation.head_projection_layer.dropout_rate = 0.25\n",
      "I0103 10:46:26.687784 139963894171456 params.py:248] model.dependency_relation.dependency_projection_layer.in_features = 1024\n",
      "I0103 10:46:26.688549 139963894171456 params.py:248] model.dependency_relation.dependency_projection_layer.out_features = 128\n",
      "I0103 10:46:26.688790 139963894171456 params.py:248] model.dependency_relation.dependency_projection_layer.activation = tanh\n",
      "I0103 10:46:26.689283 139963894171456 params.py:248] type = tanh\n",
      "I0103 10:46:26.689776 139963894171456 params.py:248] model.dependency_relation.dependency_projection_layer.dropout_rate = 0.25\n",
      "I0103 10:46:26.695935 139963894171456 params.py:248] model.regularizer.regexes.0.1.type = l2\n",
      "I0103 10:46:26.696793 139963894171456 params.py:248] model.regularizer.regexes.0.1.alpha = 1e-06\n",
      "I0103 10:46:26.698111 139963894171456 params.py:248] model.regularizer.regexes.1.1.type = l2\n",
      "I0103 10:46:26.698904 139963894171456 params.py:248] model.regularizer.regexes.1.1.alpha = 1e-06\n",
      "I0103 10:46:26.699606 139963894171456 params.py:248] model.regularizer.regexes.2.1.type = l2\n",
      "I0103 10:46:26.700381 139963894171456 params.py:248] model.regularizer.regexes.2.1.alpha = 1e-06\n",
      "I0103 10:46:26.701236 139963894171456 params.py:248] model.regularizer.regexes.3.1.type = l2\n",
      "I0103 10:46:26.702003 139963894171456 params.py:248] model.regularizer.regexes.3.1.alpha = 1e-05\n",
      "I0103 10:46:28.104316 139963894171456 archival.py:211] removing temporary unarchived model dir at /tmp/tmpog9bag7p\n",
      "reading instances: 5337it [39:11,  2.27it/s]\n",
      "\n",
      "real\t40m15,785s\n",
      "user\t140m16,111s\n",
      "sys\t1m7,367s\n"
     ]
    }
   ],
   "source": [
    "!time combo --mode predict --model_path nlp_task/source/polish-herbert-base.tar.gz --input_file nlp_task/predictions/file1.conllu --output_file nlp_task/predictions/combo.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\tDziecka\tdziecko\tNOUN\tsubst:sg:gen:n:col\tCase=Gen|Gender=Neut|NumType=Sets|Number=Sing\t8\tnmod:flat\t_\tSpaceAfter=No\r\n",
      "10\t.\t.\tPUNCT\tinterp\tPunctType=Peri\t1\tpunct\t_\t_\r\n",
      "\r\n",
      "# sent_id = 5335\r\n",
      "# text = Reprodukcja cyfrowa wykonana przez Fundację Nowoczesna Polska.\r\n",
      "1\tReprodukcja\treprodukcja\tNOUN\tsubst:sg:nom:f\tCase=Nom|Gender=Fem|Number=Sing\t0\troot\t_\t_\r\n",
      "2\tcyfrowa\tcyfrowy\tADJ\tadj:sg:nom:f:pos\tCase=Nom|Degree=Pos|Gender=Fem|Number=Sing\t1\tamod\t_\t_\r\n",
      "3\twykonana\twykonać\tADJ\tppas:sg:nom:f:perf:aff\tAspect=Perf|Case=Nom|Gender=Fem|Number=Sing|Polarity=Pos|VerbForm=Part|Voice=Pass\t1\tacl\t_\t_\r\n",
      "4\tprzez\tprzez\tADP\tprep:acc:nwok\tAdpType=Prep|Variant=Short\t5\tcase\t_\t_\r\n",
      "5\tFundację\tfundacja\tNOUN\tsubst:sg:acc:f\tCase=Acc|Gender=Fem|Number=Sing\t3\tobl:agent\t_\t_\r\n",
      "6\tNowoczesna\tnowoczesny\tADJ\tadj:sg:acc:f:pos\tCase=Acc|Degree=Pos|Gender=Fem|Number=Sing\t5\tamod:flat\t_\t_\r\n",
      "7\tPolska\tPolska\tADJ\tadj:sg:acc:f:pos\tCase=Acc|Degree=Pos|Gender=Fem|Number=Sing\t5\tamod:flat\t_\tSpaceAfter=No\r\n",
      "8\t.\t.\tPUNCT\tinterp\tPunctType=Peri\t1\tpunct\t_\tSpaceAfter=No\r\n",
      "\r\n",
      "# sent_id = 5336\r\n",
      "# text = Opracowanie redakcyjne i przypisy: Paulina Choromańska, Agnieszka Gąsior, Wojciech Kotwica, Dorota Kowalska, Paweł Kozioł.\r\n",
      "1\tOpracowanie\topracować\tNOUN\tger:sg:nom:n:perf:aff\tAspect=Perf|Case=Nom|Gender=Neut|Number=Sing|Polarity=Pos|VerbForm=Vnoun\t0\troot\t_\t_\r\n",
      "2\tredakcyjne\tredakcyjny\tADJ\tadj:sg:nom:n:pos\tCase=Nom|Degree=Pos|Gender=Neut|Number=Sing\t1\tamod\t_\t_\r\n",
      "3\ti\ti\tCCONJ\tconj\t_\t4\tcc\t_\t_\r\n",
      "4\tprzypisy\tprzypis\tNOUN\tsubst:pl:nom:m3\tAnimacy=Inan|Case=Nom|Gender=Masc|Number=Plur\t1\tconj\t_\tSpaceAfter=No\r\n",
      "5\t:\t:\tPUNCT\tinterp\tPunctType=Colo\t4\tpunct\t_\t_\r\n",
      "6\tPaulina\tPaulina\tPROPN\tsubst:sg:nom:f\tCase=Nom|Gender=Fem|Number=Sing\t1\tnmod\t_\t_\r\n",
      "7\tChoromańska\tChoromańska\tPROPN\tsubst:sg:nom:f\tCase=Nom|Gender=Fem|Number=Sing\t6\tflat\t_\tSpaceAfter=No\r\n",
      "8\t,\t,\tPUNCT\tinterp\tPunctType=Comm\t9\tpunct\t_\t_\r\n",
      "9\tAgnieszka\tAgnieszka\tPROPN\tsubst:sg:nom:f\tCase=Nom|Gender=Fem|Number=Sing\t6\tconj\t_\t_\r\n",
      "10\tGąsior\tGąsior\tPROPN\tsubst:sg:nom:f\tCase=Nom|Gender=Fem|Number=Sing\t9\tflat\t_\tSpaceAfter=No\r\n",
      "11\t,\t,\tPUNCT\tinterp\tPunctType=Comm\t12\tpunct\t_\t_\r\n",
      "12\tWojciech\tWojciech\tPROPN\tsubst:sg:nom:m1\tAnimacy=Hum|Case=Nom|Gender=Masc|Number=Sing\t6\tconj\t_\t_\r\n",
      "13\tKotwica\tKotwica\tPROPN\tsubst:sg:nom:m1\tAnimacy=Hum|Case=Nom|Gender=Masc|Number=Sing\t12\tflat\t_\tSpaceAfter=No\r\n",
      "14\t,\t,\tPUNCT\tinterp\tPunctType=Comm\t15\tpunct\t_\t_\r\n",
      "15\tDorota\tDorota\tPROPN\tsubst:sg:nom:f\tCase=Nom|Gender=Fem|Number=Sing\t6\tconj\t_\t_\r\n",
      "16\tKowalska\tKowalska\tPROPN\tsubst:sg:nom:f\tCase=Nom|Gender=Fem|Number=Sing\t15\tflat\t_\tSpaceAfter=No\r\n",
      "17\t,\t,\tPUNCT\tinterp\tPunctType=Comm\t18\tpunct\t_\t_\r\n",
      "18\tPaweł\tPaweł\tPROPN\tsubst:sg:nom:m1\tAnimacy=Hum|Case=Nom|Gender=Masc|Number=Sing\t6\tconj\t_\t_\r\n",
      "19\tKozioł\tKozioł\tPROPN\tsubst:sg:nom:m1\tAnimacy=Hum|Case=Nom|Gender=Masc|Number=Sing\t18\tflat\t_\tSpaceAfter=No\r\n",
      "20\t.\t.\tPUNCT\tinterp\tPunctType=Peri\t1\tpunct\t_\tSpaceAfter=No\r\n",
      "\r\n",
      "# sent_id = 5337\r\n",
      "# text = ISBN 978-83-288-2376-1\r\n",
      "1\tISBN\tISBN\tNOUN\tsubst:sg:nom:m3\tAnimacy=Inan|Case=Nom|Gender=Masc|Number=Sing\t0\troot\t_\t_\r\n",
      "2\t978\t978\tX\tdig\tNumForm=Digit\t1\tamod\t_\tSpaceAfter=No\r\n",
      "3\t-\t-\tPUNCT\tinterp\tPunctType=Hyph\t6\tpunct\t_\tSpaceAfter=No\r\n",
      "4\t83\t83\tX\tdig\tNumForm=Digit\t2\tfixed\t_\tSpaceAfter=No\r\n",
      "5\t-\t-\tPUNCT\tinterp\tPunctType=Hyph\t6\tpunct\t_\tSpaceAfter=No\r\n",
      "6\t288\t288\tX\tdig\tNumForm=Digit\t2\tfixed\t_\tSpaceAfter=No\r\n",
      "7\t-\t-\tPUNCT\tinterp\tPunctType=Hyph\t8\tpunct\t_\tSpaceAfter=No\r\n",
      "8\t2376\t2376\tX\tdig\tNumForm=Digit\t6\tconj\t_\tSpaceAfter=No\r\n",
      "9\t-\t-\tPUNCT\tinterp\tPunctType=Hyph\t8\tpunct\t_\tSpaceAfter=No\r\n",
      "10\t1\t1\tX\tdig\tNumForm=Digit\t6\tconj\t_\tSpaceAfter=No\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! tail -n 50 nlp_task/predictions/combo.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
