{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words (BoW) - technika uproszczonej reprezentacji tekstu. Polega na przekształeceniu sekwencji segmentów do policzonego zbioru segmentów. Kolejność segmentów nie ma znaczenia. Głównym zastosowaniem jest odwzorowanie podobieństwa znaczeniowego/miar podobieństwa dokumentów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Agnieszka ma nowy samochód\",\n",
    "    \"Agnieszka z Warszawy ma piękny samochód\",\n",
    "    \"Arek ma nowy rower\",\n",
    "    \"Samochód Agnieszki jest nowy\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podział tekstu na tokeny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Agnieszka', 'ma', 'nowy', 'samochód'],\n",
       " ['Agnieszka', 'z', 'Warszawy', 'ma', 'piękny', 'samochód'],\n",
       " ['Arek', 'ma', 'nowy', 'rower'],\n",
       " ['Samochód', 'Agnieszki', 'jest', 'nowy']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_orth = [tokenizer.tokenize(text) for text in texts]\n",
    "docs_orth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przekształcenie listy do słownika wraz z liczbą wystąpień poszczególnych elementów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FreqDist({'Agnieszka': 1, 'ma': 1, 'nowy': 1, 'samochód': 1}),\n",
       " FreqDist({'Agnieszka': 1, 'z': 1, 'Warszawy': 1, 'ma': 1, 'piękny': 1, 'samochód': 1}),\n",
       " FreqDist({'Arek': 1, 'ma': 1, 'nowy': 1, 'rower': 1}),\n",
       " FreqDist({'Samochód': 1, 'Agnieszki': 1, 'jest': 1, 'nowy': 1})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "freq_orth = [FreqDist(doc) for doc in docs_orth]\n",
    "freq_orth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miara podobieństwa zbiorów w oparciu o indeks Jaccarda.\n",
    "* przecięcie zbiorów = zbiór lematów występujących w obu zbiorach jednoczesnie\n",
    "* suma zbiorów = zbiów wszystkich lematów występujących w obu zbiorach\n",
    "* J(A,B) = |przecięcie zbiorów| / |suma zbiorów|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Miara podobieństwa w oparciu o indeks Jaccarda](https://miro.medium.com/max/770/1*XiLRKr_Bo-VdgqVI-SvSQg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funkcja przyjmuje 2 argumenty - reprezentacje dwóch dokumentów\n",
    "def bow_similarity(bow1: FreqDist, bow2: FreqDist) -> float:\n",
    "    # suma zbiorów \n",
    "    bow_all = bow1 + bow2\n",
    "    # przecięcie zbiorów. Iteracja po elementach sumy zbiorów  i sprawdzanie czy występuje w każdym z dokumentów\n",
    "    bow_common = [word for word in bow_all.keys() if word in bow1 and word in bow2]\n",
    "    # jeżeli liczność sumy zbiorów jest zerowa to zwracamy zero\n",
    "    # jeżeli liczność sumy zbiorów nie jest zerowa to postępujemy zgodnie ze wzorem Jaccarda: J(A,B)  \n",
    "    return 0 if len(bow_all) == 0 else len(bow_common) / len(bow_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agnieszka ma nowy samochód \n",
      "\n",
      "0.4286  Agnieszka z Warszawy ma piękny samochód\n",
      "0.3333  Arek ma nowy rower\n",
      "0.1429  Samochód Agnieszki jest nowy\n"
     ]
    }
   ],
   "source": [
    "print(texts[0], \"\\n\")\n",
    "for n in range(1, 4):\n",
    "    sim = bow_similarity(freq_orth[0], freq_orth[n])\n",
    "    print(f\"{sim:<6.4}  {texts[n]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W powyższym przykładzie prównujemy miarę podobieństwa pomiędzy zdaniem \"Agnieszka ma nowy samochód\", a pozostałymi.\n",
    "Jak widzimy zdanie \"Arek ma nowy rower\" otrzymało lepszy ranking niż \"Samochód Agnieszki jest nowy\". Dzieje się tak dlatego, ponieważ indeks Jaccarda nie wskazuje, które zdania są bliższe pod względem semantycznym, ale które współdzielą więcej form tekstowych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sprawdź również BoW z użyciem biblioteki spaCy i wektoryzację z użyciem biblioteki sklearn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
