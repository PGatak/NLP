{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('pl_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Krzysztof Kowalski żył w latach 1954-2015.\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sprawdź ilość tokenów i wypisz je"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba tokenów: 9\n",
      "Krzysztof\n",
      "Kowalski\n",
      "żył\n",
      "w\n",
      "latach\n",
      "1954\n",
      "-\n",
      "2015\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "print(f\"Liczba tokenów: {len(doc)}\")\n",
    "print(*doc, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dodatkowe atrybuty\n",
    "* ```token.text lub token.orth = tekst tokena```\n",
    "* ```token.whitespace_ = zmienna, która zawiera białe znaki występujące po tokenie```\n",
    "* ```token.text_with_ws = połączenie pola token.text + token.whitespace_```\n",
    "* ```token.orth = identyfikator w słowniku```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krzysztof       Krzysztof       1 Krzysztof    4980819922259476092 \n",
      "Kowalski        Kowalski        1 Kowalski     6064732076458680349 \n",
      "żył             żył             1 żył          17463462916583227822 \n",
      "w               w               1 w            260667111241363922 \n",
      "latach          latach          1 latach       8838775445208903749 \n",
      "1954            1954            0 1954         3806804293181247087 \n",
      "-               -               0 -            9153284864653046197 \n",
      "2015            2015            0 2015         5268047934427719614 \n",
      ".               .               0 .            12646065887601541794 \n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f\"{token.text:<15} {token.orth_:<15} {len(token.whitespace_)} {token.text_with_ws:<12} {token.orth} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Krzysztof'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.vocab[4980819922259476092].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text_with_ws tworzy doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join([t.text_with_ws for t in doc]) == text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### potok przetwarzania\n",
    "* tagger = tagowanie\n",
    "* parser = parsowanie\n",
    "* ner = rozpoznawanie nazw własnych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipe_names) # jakie moduły występują w modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krzysztof\n",
      "Kowalski\n",
      "żył\n",
      "w\n",
      "latach\n",
      "1954\n",
      "-\n",
      "2015\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text, disable=[\"tagger\", \"parser\", \"ner\"]) # wyłączenie modułów z potoku przetwarzania\n",
    "\n",
    "print(*doc, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### porównaj czas wykonywania z włączonymi/wyłączonymi modułami potoku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "CPU times: user 4.91 s, sys: 0 ns, total: 4.91 s\n",
      "Wall time: 4.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for n in range(200):\n",
    "    doc = nlp(text)\n",
    "print(len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "CPU times: user 6.56 ms, sys: 124 µs, total: 6.69 ms\n",
      "Wall time: 5.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for n in range(200):\n",
    "    doc = nlp(text, disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "print(len(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reguły użyte podczas tokenizacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:00                     TOKEN\n",
      "12/10/2020                TOKEN\n",
      "imie.nazwisko@gmail.com   URL_MATCH\n",
      "#                         PREFIX\n",
      "hashtag                   TOKEN\n",
      "@user01                   TOKEN\n",
      ":-)                       SPECIAL-1\n",
      "adres.pl                  URL_MATCH\n",
      ".                         SUFFIX\n"
     ]
    }
   ],
   "source": [
    "for rule, token in nlp.tokenizer.explain(\"22:00 12/10/2020 imie.nazwisko@gmail.com #hashtag @user01 :-) adres.pl .\"):\n",
    "    print(f\"{token:<25} {rule}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wyświetl emotikony tekstowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t \n",
      "   \") ' '' (*_*) (-8 (-: (-; (-_-) (._.) (: (; (= (>_<) (^_^) (o: (¬_¬) (ಠ_ಠ) (╯°□°）╯︵┻━┻ )-: ): -_- -__- 0.0 0.o 0_0 0_o 8) 8-) 8-D 8D :'( :') :'-( :'-) :( :(( :((( :() :) :)) :))) :* :-( :-(( :-((( :-) :-)) :-))) :-* :-/ :-0 :-3 :-> :-D :-O :-P :-X :-] :-o :-p :-x :-| :-} :/ :0 :1 :3 :> :D :O :P :X :] :o :o) :p :x :| :} ;) ;-) ;-D ;D ;_; <.< </3 <3 <33 <333 <space> =( =) =/ =3 =D =| >.< >.> >:( >:o ><(((*> @_@ C++ O.O O.o O_O O_o V.V V_V XD XDD [-: [: \\\") \\n \\t ^_^ ^__^ ^___^ o.0 o.O o.o o_0 o_O o_o v.v v_v xD xDD   ¯\\(ツ)/¯ ಠ_ಠ ಠ︵ಠ —\n"
     ]
    }
   ],
   "source": [
    "print(*nlp.tokenizer.rules)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
